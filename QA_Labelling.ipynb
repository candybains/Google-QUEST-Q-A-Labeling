{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import transformers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pickle  \n",
    "import random\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from os.path import join as path_join\n",
    "from numpy.random import seed\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder, RobustScaler, KBinsDiscretizer, QuantileTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold, GroupKFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor, RANSACRegressor\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'E:/Prabhkirat/Python/google-quest-challenge/'\n",
    "metas_dir = ''\n",
    "sub_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0121\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "import datetime\n",
    "todate = datetime.date.today().strftime(\"%m%d\")\n",
    "\n",
    "print(todate)\n",
    "nfolds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words\n",
    "def word_count(xstring):\n",
    "    return xstring.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_corr(y_true, y_pred):\n",
    "        if np.ndim(y_pred) == 2:\n",
    "            corr = np.mean([stats.spearmanr(y_true[:, i], y_pred[:, i])[0] for i in range(y_true.shape[1])])\n",
    "        else:\n",
    "            corr = stats.spearmanr(y_true, y_pred)[0]\n",
    "        return corr\n",
    "    \n",
    "custom_scorer = make_scorer(spearman_corr, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_vectors(string_list, batch_size=64):\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    fin_features = []\n",
    "    for data in chunks(string_list, batch_size):\n",
    "        tokenized = []\n",
    "        for x in data:\n",
    "            x = \" \".join(x.strip().split()[:300])\n",
    "            tok = tokenizer.encode(x, add_special_tokens=True)\n",
    "            tokenized.append(tok[:512])\n",
    "\n",
    "        max_len = 512\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded).to(DEVICE)\n",
    "        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "        fin_features.append(features)\n",
    "\n",
    "    fin_features = np.vstack(fin_features)\n",
    "    return fin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.read_csv(data_dir + 'train.csv')\n",
    "xtest = pd.read_csv(data_dir + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice', \n",
    "               'question_type_compare', 'question_type_consequence', \n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful', \n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in ['question_title', 'question_body', 'answer']:\n",
    "    newname = colname + '_word_len'\n",
    "    xtrain[newname] = xtrain[colname].str.split().str.len()\n",
    "    xtest[newname] = xtest[colname].str.split().str.len()\n",
    "    #print(xtrain[newname])\n",
    "del newname, colname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in ['question', 'answer']:\n",
    "\n",
    "    # check for nonames, i.e. users with logins like user12389\n",
    "    xtrain['is_'+colname+'_no_name_user'] = xtrain[colname +'_user_name'].str.contains('^user\\d+$') + 0\n",
    "    xtest['is_'+colname+'_no_name_user'] = xtest[colname +'_user_name'].str.contains('^user\\d+$') + 0\n",
    "    \n",
    "\n",
    "colname = 'answer'\n",
    "# check lexical diversity (unique words count vs total )\n",
    "xtrain[colname+'_div'] = xtrain[colname].apply(lambda s: len(set(s.split())) / len(s.split()) )\n",
    "xtest[colname+'_div'] = xtest[colname].apply(lambda s: len(set(s.split())) / len(s.split()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               [photo, stackexchange, com]\n",
      "1                 [rpg, stackexchange, com]\n",
      "2         [electronics, stackexchange, com]\n",
      "3             [judaism, stackexchange, com]\n",
      "4       [graphicdesign, stackexchange, com]\n",
      "                       ...                 \n",
      "6074         [bicycles, stackexchange, com]\n",
      "6075         [bicycles, stackexchange, com]\n",
      "6076             [unix, stackexchange, com]\n",
      "6077                    [mathoverflow, net]\n",
      "6078              [diy, stackexchange, com]\n",
      "Name: domcom, Length: 6079, dtype: object\n",
      "0       3\n",
      "1       3\n",
      "2       3\n",
      "3       3\n",
      "4       3\n",
      "       ..\n",
      "6074    3\n",
      "6075    3\n",
      "6076    3\n",
      "6077    2\n",
      "6078    3\n",
      "Name: dom_cnt, Length: 6079, dtype: int64\n",
      "0               [photo, stackexchange, com, none, none]\n",
      "1                 [rpg, stackexchange, com, none, none]\n",
      "2         [electronics, stackexchange, com, none, none]\n",
      "3             [judaism, stackexchange, com, none, none]\n",
      "4       [graphicdesign, stackexchange, com, none, none]\n",
      "                             ...                       \n",
      "6074         [bicycles, stackexchange, com, none, none]\n",
      "6075         [bicycles, stackexchange, com, none, none]\n",
      "6076             [unix, stackexchange, com, none, none]\n",
      "6077                    [mathoverflow, net, none, none]\n",
      "6078              [diy, stackexchange, com, none, none]\n",
      "Name: domcom, Length: 6079, dtype: object\n",
      "0               photo\n",
      "1                 rpg\n",
      "2         electronics\n",
      "3             judaism\n",
      "4       graphicdesign\n",
      "            ...      \n",
      "6074         bicycles\n",
      "6075         bicycles\n",
      "6076             unix\n",
      "6077     mathoverflow\n",
      "6078              diy\n",
      "Name: dom_0, Length: 6079, dtype: object\n",
      "0       stackexchange\n",
      "1       stackexchange\n",
      "2       stackexchange\n",
      "3       stackexchange\n",
      "4       stackexchange\n",
      "            ...      \n",
      "6074    stackexchange\n",
      "6075    stackexchange\n",
      "6076    stackexchange\n",
      "6077              net\n",
      "6078    stackexchange\n",
      "Name: dom_1, Length: 6079, dtype: object\n",
      "0        com\n",
      "1        com\n",
      "2        com\n",
      "3        com\n",
      "4        com\n",
      "        ... \n",
      "6074     com\n",
      "6075     com\n",
      "6076     com\n",
      "6077    none\n",
      "6078     com\n",
      "Name: dom_2, Length: 6079, dtype: object\n",
      "0       none\n",
      "1       none\n",
      "2       none\n",
      "3       none\n",
      "4       none\n",
      "        ... \n",
      "6074    none\n",
      "6075    none\n",
      "6076    none\n",
      "6077    none\n",
      "6078    none\n",
      "Name: dom_3, Length: 6079, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## domain components\n",
    "xtrain['domcom'] = xtrain['question_user_page'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\n",
    "xtest['domcom'] = xtest['question_user_page'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\n",
    "print(xtrain['domcom'])\n",
    "# count components\n",
    "xtrain['dom_cnt'] = xtrain['domcom'].apply(lambda s: len(s))\n",
    "xtest['dom_cnt'] = xtest['domcom'].apply(lambda s: len(s))\n",
    "print(xtrain['dom_cnt'])\n",
    "# extend length\n",
    "xtrain['domcom'] = xtrain['domcom'].apply(lambda s: s + ['none', 'none'])\n",
    "xtest['domcom'] = xtest['domcom'].apply(lambda s: s + ['none', 'none'])\n",
    "print(xtrain['domcom'])\n",
    "# components\n",
    "for ii in range(0,4):\n",
    "    xtrain['dom_'+str(ii)] = xtrain['domcom'].apply(lambda s: s[ii])\n",
    "    xtest['dom_'+str(ii)] = xtest['domcom'].apply(lambda s: s[ii])\n",
    "    print(xtrain['dom_'+str(ii)])\n",
    "# clean up\n",
    "xtrain.drop('domcom', axis = 1, inplace = True)\n",
    "xtest.drop('domcom', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [After, playing, around, macro, photography, o...\n",
      "1       [I, trying, understand, kinds, places, spam, v...\n",
      "2       [I'm, working, PCB, through-hole, components, ...\n",
      "3       [An, affidavit,, understand,, basically, signe...\n",
      "4       [I, trying, make, binary, image., I, want, loo...\n",
      "                              ...                        \n",
      "6074    [I, curious, anyone, uses, skiing, helmet, win...\n",
      "6075    [I, road, bike, front, brake, wears, lot, brak...\n",
      "6076    [I'm, tailing, log, file, using, tail, -f, mes...\n",
      "6077    [What, people's, views, this?, To, specific:, ...\n",
      "6078    [Newbie, question., Why, there's, bazillion, d...\n",
      "Name: q_words, Length: 6079, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# shared elements\n",
    "xtrain['q_words'] = xtrain['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n",
    "print(xtrain['q_words'])\n",
    "xtrain['a_words'] = xtrain['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n",
    "xtrain['qa_word_overlap'] = xtrain.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n",
    "xtrain['qa_word_overlap_norm1'] = xtrain.apply(lambda s: s['qa_word_overlap']/(1 + len(s['a_words'])), axis = 1)\n",
    "xtrain['qa_word_overlap_norm2'] = xtrain.apply(lambda s: s['qa_word_overlap']/(1 + len(s['q_words'])), axis = 1)\n",
    "xtrain.drop(['q_words', 'a_words'], axis = 1, inplace = True)\n",
    "\n",
    "xtest['q_words'] = xtest['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n",
    "xtest['a_words'] = xtest['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n",
    "xtest['qa_word_overlap'] = xtest.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n",
    "xtest['qa_word_overlap_norm1'] = xtest.apply(lambda s: s['qa_word_overlap']/(1 + len(s['a_words'])), axis = 1)\n",
    "xtest['qa_word_overlap_norm2'] = xtest.apply(lambda s: s['qa_word_overlap']/(1 + len(s['q_words'])), axis = 1)\n",
    "xtest.drop(['q_words', 'a_words'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of characters in the text ##\n",
    "xtrain[\"question_title_num_chars\"] = xtrain[\"question_title\"].apply(lambda x: len(str(x)))\n",
    "xtest[\"question_title_num_chars\"] = xtest[\"question_title\"].apply(lambda x: len(str(x)))\n",
    "xtrain[\"question_body_num_chars\"] = xtrain[\"question_body\"].apply(lambda x: len(str(x)))\n",
    "xtest[\"question_body_num_chars\"] = xtest[\"question_body\"].apply(lambda x: len(str(x)))\n",
    "xtrain[\"answer_num_chars\"] = xtrain[\"answer\"].apply(lambda x: len(str(x)))\n",
    "xtest[\"answer_num_chars\"] = xtest[\"answer\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "xtrain[\"question_title_num_stopwords\"] = xtrain[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "xtest[\"question_title_num_stopwords\"] = xtest[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "xtrain[\"question_body_num_stopwords\"] = xtrain[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "xtest[\"question_body_num_stopwords\"] = xtest[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "xtrain[\"answer_num_stopwords\"] = xtrain[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "xtest[\"answer_num_stopwords\"] = xtest[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "xtrain[\"question_title_num_punctuations\"] =xtrain['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "xtest[\"question_title_num_punctuations\"] =xtest['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "xtrain[\"question_body_num_punctuations\"] =xtrain['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "xtest[\"question_body_num_punctuations\"] =xtest['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "xtrain[\"answer_num_punctuations\"] =xtrain['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "xtest[\"answer_num_punctuations\"] =xtest['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "xtrain[\"question_title_num_words_upper\"] = xtrain[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "xtest[\"question_title_num_words_upper\"] = xtest[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "xtrain[\"question_body_num_words_upper\"] = xtrain[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "xtest[\"question_body_num_words_upper\"] = xtest[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "xtrain[\"answer_num_words_upper\"] = xtrain[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "xtest[\"answer_num_words_upper\"] = xtest[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-377c8822e5f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mcurr_train_emb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"outputs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mind\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embed' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings_train = {}\n",
    "embeddings_test = {}\n",
    "for text in ['question_title', 'question_body', 'answer']:\n",
    "    train_text = xtrain[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    test_text = xtest[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    \n",
    "    curr_train_emb = []\n",
    "    curr_test_emb = []\n",
    "    batch_size = 4\n",
    "    ind = 0\n",
    "    while ind*batch_size < len(train_text):\n",
    "        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1\n",
    "        \n",
    "    ind = 0\n",
    "    while ind*batch_size < len(test_text):\n",
    "        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1    \n",
    "        \n",
    "    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n",
    "    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
    "\n",
    "    print(text)\n",
    "    \n",
    "del embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_char = 5000\n",
    "limit_word = 25000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['qa_id', 'question_title', 'question_body', 'question_user_name',\n",
      "       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',\n",
      "       'url', 'category', 'host', 'question_title_word_len',\n",
      "       'question_body_word_len', 'answer_word_len', 'is_question_no_name_user',\n",
      "       'is_answer_no_name_user', 'answer_div', 'dom_cnt', 'dom_0', 'dom_1',\n",
      "       'dom_2', 'dom_3', 'qa_word_overlap', 'qa_word_overlap_norm1',\n",
      "       'qa_word_overlap_norm2', 'question_title_num_chars',\n",
      "       'question_body_num_chars', 'answer_num_chars',\n",
      "       'question_title_num_stopwords', 'question_body_num_stopwords',\n",
      "       'answer_num_stopwords', 'question_title_num_punctuations',\n",
      "       'question_body_num_punctuations', 'answer_num_punctuations',\n",
      "       'question_title_num_words_upper', 'question_body_num_words_upper',\n",
      "       'answer_num_words_upper'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cols = xtrain.loc[:, ~xtrain.columns.isin(target_cols)].columns.tolist()\n",
    "X = xtrain[cols]\n",
    "y = xtrain[target_cols].values\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_col = 'question_title'\n",
    "title_transformer = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n",
    "                             binary = False, use_idf = True, smooth_idf = False,\n",
    "                             ngram_range = (1,2), stop_words = 'english', \n",
    "                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n",
    "])\n",
    "        \n",
    "title_transformer2 = Pipeline([\n",
    " ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n",
    "    strip_accents='unicode', analyzer='char',\n",
    "    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n",
    "])\n",
    "\n",
    "\n",
    "body_col = 'question_body'\n",
    "body_transformer = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n",
    "                             binary = False, use_idf = True, smooth_idf = False,\n",
    "                             ngram_range = (1,2), stop_words = 'english', \n",
    "                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n",
    "])\n",
    "\n",
    "\n",
    "body_transformer2 = Pipeline([\n",
    " ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n",
    "    strip_accents='unicode', analyzer='char',\n",
    "    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n",
    "])\n",
    "\n",
    "answer_col = 'answer'\n",
    "\n",
    "answer_transformer = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n",
    "                             binary = False, use_idf = True, smooth_idf = False,\n",
    "                             ngram_range = (1,2), stop_words = 'english', \n",
    "                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n",
    "])\n",
    "\n",
    "answer_transformer2 = Pipeline([\n",
    " ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n",
    "    strip_accents='unicode', analyzer='char',\n",
    "    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n",
    "])\n",
    "\n",
    "num_cols = [\n",
    "    'question_title_word_len', 'question_body_word_len', 'answer_word_len', 'answer_div',\n",
    "    'question_title_num_chars','question_body_num_chars','answer_num_chars',\n",
    "    'question_title_num_stopwords','question_body_num_stopwords','answer_num_stopwords',\n",
    "    'question_title_num_punctuations','question_body_num_punctuations','answer_num_punctuations',\n",
    "    'question_title_num_words_upper','question_body_num_words_upper','answer_num_words_upper'\n",
    "]\n",
    "\n",
    "num_transformer = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scale', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "\n",
    "cat_cols = [\n",
    "    'dom_0', \n",
    "    'dom_1', \n",
    "    'dom_2', \n",
    "    'dom_3',     \n",
    "    'category', \n",
    "    'is_question_no_name_user',\n",
    "    'is_answer_no_name_user',\n",
    "    'dom_cnt'\n",
    "]\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='')),\n",
    "    ('encode', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('title', title_transformer, title_col),\n",
    "        ('title2', title_transformer2, title_col),\n",
    "        ('body', body_transformer, body_col),\n",
    "        ('body2', body_transformer2, body_col),\n",
    "        ('answer', answer_transformer, answer_col),\n",
    "        ('answer2', answer_transformer2, answer_col),\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('estimator',Ridge(random_state=RANDOM_STATE))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_as = np.zeros((y.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits = nfolds, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "param_grid = {\n",
    "    'estimator': [\n",
    "        Ridge(random_state=RANDOM_STATE),\n",
    "        \n",
    "    ],\n",
    "    'estimator__alpha': [1, 5, 20],\n",
    "    \n",
    "    'preprocessor__title__tfidf__lowercase': [False],\n",
    "    'preprocessor__title__tfidf__max_df': [0.3],\n",
    "    'preprocessor__title__tfidf__min_df': [1],\n",
    "    'preprocessor__title__tfidf__binary': [False],\n",
    "    'preprocessor__title__tfidf__use_idf': [True],\n",
    "    'preprocessor__title__tfidf__smooth_idf': [False],\n",
    "    'preprocessor__title__tfidf__sublinear_tf': [False],\n",
    "    'preprocessor__title__tfidf__ngram_range': [(1, 2)], \n",
    "    'preprocessor__title__tfidf__stop_words': ['english'],\n",
    "    'preprocessor__title__tfidf__token_pattern': ['(?u)\\\\b\\\\w+\\\\b'],\n",
    "    \n",
    "    'preprocessor__body__tfidf__lowercase': [False],\n",
    "    'preprocessor__body__tfidf__max_df': [0.3],\n",
    "    'preprocessor__body__tfidf__min_df': [1],\n",
    "    'preprocessor__body__tfidf__binary': [False],\n",
    "    'preprocessor__body__tfidf__use_idf': [False],\n",
    "    'preprocessor__body__tfidf__smooth_idf': [False],\n",
    "    'preprocessor__body__tfidf__sublinear_tf': [False],\n",
    "    'preprocessor__body__tfidf__ngram_range': [(1, 2)],\n",
    "    'preprocessor__body__tfidf__stop_words': ['english'],\n",
    "    'preprocessor__body__tfidf__token_pattern': ['(?u)\\\\b\\\\w+\\\\b'],\n",
    "\n",
    "    'preprocessor__num__impute__strategy': ['constant'],\n",
    "    'preprocessor__num__scale': [PowerTransformer()],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.9min finished\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "\n",
    "for ii in range(0, y.shape[1]):\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, scoring=custom_scorer, \n",
    "                           cv=cv, n_jobs=-1, refit=True, return_train_score=True, verbose=2)\n",
    "\n",
    "    grid_search.fit(X, y[:,ii])\n",
    "    \n",
    "    \n",
    "    grid_search.best_score_, grid_search.best_params_, grid_search.cv_results_\n",
    "    best_estimator = clone(grid_search.best_estimator_)\n",
    "    model_list.append(best_estimator)\n",
    "    vector_as[ii] = model_list[ii].steps[1][1].alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pda = pd.DataFrame(vector_as)\n",
    "pda.columns = ['alpha']\n",
    "pda.to_csv('E:/Prabhkirat/Python/alphas_vector.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train = xtrain['qa_id']\n",
    "ytrain = xtrain[target_cols]\n",
    "xtrain.drop(target_cols + ['qa_id'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "id_test = xtest['qa_id'] \n",
    "xtest.drop('qa_id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = [f for f in xtrain.columns if f not in cat_cols and f not in num_cols and f not in title_col and f not in body_col and f not in answer_col]\n",
    "\n",
    "xtrain.drop(dropcols, axis = 1, inplace = True)\n",
    "xtest.drop(dropcols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvalid = np.zeros((xtrain.shape[0], len(target_cols)))\n",
    "mfull = np.zeros((xtest.shape[0], len(target_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = nfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(xtrain):\n",
    "    \n",
    "    print('---')\n",
    "    # split\n",
    "    x0, x1 = xtrain.loc[train_index], xtrain.loc[test_index]\n",
    "    y0, y1 = ytrain.loc[train_index], ytrain.loc[test_index]\n",
    "\n",
    "    for ii in range(0, ytrain.shape[1]):\n",
    "\n",
    "        # fit model\n",
    "        be = model_list[ii]\n",
    "        be.fit(x0, np.array(y0)[:,ii])\n",
    "\n",
    "        # park forecast\n",
    "        mvalid[test_index, ii] = be.predict(x1)\n",
    "        mfull[:,ii] += be.predict(xtest)/kf.n_splits\n",
    "        # print(stats.spearmanr(np.array(y1)[:,ii], mvalid[test_index, ii])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corvec = np.zeros((ytrain.shape[1],1))\n",
    "for ii in range(0, ytrain.shape[1]):\n",
    "    mvalid[:,ii] = rankdata(mvalid[:,ii])/mvalid.shape[0]\n",
    "    mfull[:,ii] = rankdata(mfull[:,ii])/mfull.shape[0]\n",
    "    \n",
    "    corvec[ii] = stats.spearmanr(ytrain[ytrain.columns[ii]], mvalid[:,ii])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37235585509384733\n"
     ]
    }
   ],
   "source": [
    "print(corvec.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prval = pd.DataFrame(mvalid)\n",
    "prval.columns = ytrain.columns\n",
    "prval['qa_id'] = id_train\n",
    "prval = prval[['qa_id'] + list(prval.columns[:-1])]\n",
    "prval.to_csv('E:/Prabhkirat/Python/'+ metas_dir + 'prval_ridge_'+todate+ '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prfull = pd.DataFrame(mfull)\n",
    "prfull.columns = ytrain.columns\n",
    "prfull['qa_id'] = id_test\n",
    "prfull = prfull[['qa_id'] + list(prfull.columns[:-1])]\n",
    "prfull.to_csv('E:/Prabhkirat/Python/'+ metas_dir + 'prfull_ridge_'+todate+ '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prfull.to_csv('E:/Prabhkirat/Python/'+ 'submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
